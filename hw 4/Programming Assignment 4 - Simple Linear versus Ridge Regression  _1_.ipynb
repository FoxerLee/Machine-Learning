{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3 - Simple Linear versus Ridge Regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your friend Bob just moved to Boston. He is a real estate agent who is trying to evaluate the prices of houses in the Boston area. He has been using a linear regression model but he wonders if he can improve his accuracy on predicting the prices for new houses. He comes to you for help as he knows that you're an expert in machine learning. \n",
    "\n",
    "As a pro, you suggest doing a *polynomial transformation*  to create a more flexible model, and performing ridge regression since having so many features compared to data points increases the variance. \n",
    "\n",
    "Bob, however, being a skeptic isn't convinced. He wants you to write a program that illustrates the difference in training and test costs for both linear and ridge regression on the same dataset. Being a good friend, you oblige and hence this assignment :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you are to explore the effects of ridge regression.  We will use a dataset that is part of the sklearn.dataset package.  Learn more at https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1:  Getting, understanding, and preprocessing the dataset\n",
    "\n",
    "We first import the standard libaries and some libraries that will help us scale the data and perform some \"feature engineering\" by transforming the data into $\\Phi_2({\\bf x})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the boston dataset from sklearn\n",
    "boston_data = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of features is:  13\n",
      "The features:  ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "The number of exampels in our dataset:  506\n",
      "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
      "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
      "  4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
      "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
      "  9.1400e+00]]\n"
     ]
    }
   ],
   "source": [
    "#  Create X and Y variables - X holding the .data and Y holding .target \n",
    "X = boston_data.data\n",
    "y = boston_data.target\n",
    "\n",
    "#  Reshape Y to be a rank 2 matrix \n",
    "y = y.reshape(X.shape[0], 1)\n",
    "\n",
    "# Observe the number of features and the number of labels\n",
    "print('The number of features is: ', X.shape[1])\n",
    "# Printing out the features\n",
    "print('The features: ', boston_data.feature_names)\n",
    "# The number of examples\n",
    "print('The number of exampels in our dataset: ', X.shape[0])\n",
    "#Observing the first 2 rows of the data\n",
    "print(X[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also create polynomial feeatures for the dataset to test linear and ridge regression on data with d = 1 and data with d = 2. Feel free to increase the # of degress and see what effect it has on the training and test error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PolynomialFeatures object with degree = 2. \n",
    "# Transform X and save it into X_2. Simply copy Y into Y_2 \n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_2 = poly.fit_transform(X)\n",
    "y_2 = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 105)\n",
      "(506, 1)\n"
     ]
    }
   ],
   "source": [
    "# the shape of X_2 and Y_2 - should be (506, 105) and (506, 1) respectively\n",
    "print(X_2.shape)\n",
    "print(y_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the get_coeff_ridge_normaleq function. Use the normal equation method.\n",
    "# TODO - Return w values\n",
    "\n",
    "def get_coeff_ridge_normaleq(X_train, y_train, alpha):\n",
    "    # use np.linalg.pinv(a)\n",
    "    #### TO-DO #####\n",
    "    if alpha == 0:\n",
    "        w = np.dot(np.linalg.pinv(X_train), y_train)\n",
    "    else:\n",
    "        I = np.eye(X_train.shape[1])\n",
    "        inv = np.dot(X_train.T, X_train) + alpha*I\n",
    "        inv = np.linalg.pinv(inv)\n",
    "        w = np.dot(inv, X_train.T)\n",
    "        w = np.dot(w, y_train)\n",
    "    ##############\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Define the evaluate_err_ridge function.\n",
    "# TODO - Return the train_error and test_error values\n",
    "def evaluate_err(X_train, X_test, y_train, y_test, w): \n",
    "    #### TO-DO #####\n",
    "    y_train_pre = np.dot(X_train, w)\n",
    "    y_test_pre = np.dot(X_test, w)\n",
    "    \n",
    "    train_rss = np.sum((y_train_pre-y_train)**2)\n",
    "    test_rss = np.sum((y_test_pre-y_test)**2)\n",
    "    \n",
    "\n",
    "    train_error = train_rss / len(X_train)\n",
    "    test_error = test_rss / len(X_test)\n",
    "    ##############\n",
    "    return train_error, test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - Finish writting the k_fold_cross_validation function. \n",
    "# TODO - Returns the average training error and average test error from the k-fold cross validation\n",
    "# use Sklearns K-Folds cross-validator: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n",
    "\n",
    "def k_fold_cross_validation(k, X, y, alpha):\n",
    "    kf = KFold(n_splits=k, random_state=21, shuffle=True)\n",
    "    total_E_val_test = 0\n",
    "    total_E_val_train = 0\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "        y_train_mean = np.mean(y_train)\n",
    "        y_train = y_train - y_train_mean\n",
    "        y_test = y_test - y_train_mean\n",
    "        # scaling the data matrix\n",
    "        scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # determine the training error and the test error\n",
    "        #### TO-DO #####\n",
    "        w = get_coeff_ridge_normaleq(X_train, y_train, alpha)\n",
    "        \n",
    "        train_error, test_error = evaluate_err(X_train, X_test, y_train, y_test, w)\n",
    "        total_E_val_test += test_error\n",
    "        total_E_val_train += train_error\n",
    "        \n",
    "    \n",
    "       ##############\n",
    "    E_val_test = total_E_val_test / k\n",
    "    E_val_train = total_E_val_train / k\n",
    "    return  E_val_test, E_val_train\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model, no polymoial transformation\n",
      "train score: 21.806183575851065\n",
      "test score: 23.63606860542818\n",
      "*****************\n",
      "Ridge regression model, no polymoial transformation\n",
      "When lamda is 10.0, train score: 21.892901157570186 | test score: 23.68858300163874\n",
      "=============\n",
      "When lamda is 31.622776601683793, train score: 22.2854440556975 | test score: 24.01784029738614\n",
      "=============\n",
      "When lamda is 100.0, train score: 23.725488384882077 | test score: 25.293852553695135\n",
      "=============\n",
      "When lamda is 316.22776601683796, train score: 28.166554098540125 | test score: 29.457296222896804\n",
      "=============\n",
      "When lamda is 1000.0, train score: 38.53214872734357 | test score: 39.48949335939871\n",
      "=============\n",
      "When lamda is 3162.2776601683795, train score: 54.0070264995545 | test score: 54.6471910779495\n",
      "=============\n",
      "When lamda is 10000.0, train score: 69.2597817132532 | test score: 69.7185175088451\n",
      "=============\n",
      "When lamda is 31622.776601683792, train score: 78.53074243629268 | test score: 78.92162110435103\n",
      "=============\n",
      "When lamda is 100000.0, train score: 82.40325012491367 | test score: 82.7724042894599\n",
      "=============\n",
      "When lamda is 316227.7660168379, train score: 83.75514887733652 | test score: 84.11748625821778\n",
      "=============\n",
      "When lamda is 1000000.0, train score: 84.19680334862025 | test score: 84.5569941832673\n",
      "=============\n",
      "When lamda is 3162277.6601683795, train score: 84.33793107733854 | test score: 84.69744416333866\n",
      "=============\n",
      "When lamda is 10000000.0, train score: 84.38270764054002 | test score: 84.74200651304514\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "E_val_test, E_val_train = k_fold_cross_validation(10, X, y, 0.0)\n",
    "print(\"Linear regression model, no polymoial transformation\")\n",
    "print(\"train score: {}\".format(E_val_train))\n",
    "print(\"test score: {}\".format(E_val_test))\n",
    "\n",
    "print(\"*****************\")\n",
    "alpha = np.logspace(1, 7, num=13)\n",
    "print(\"Ridge regression model, no polymoial transformation\")\n",
    "for a in alpha:\n",
    "    E_val_test, E_val_train = k_fold_cross_validation(10, X, y, a)\n",
    "    print(\"When lamda is {}, train score: {} | test score: {}\".format(a, E_val_train, E_val_test))\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression model, with polymoial transformation\n",
      "train score: 5.808820816012461\n",
      "test score: 11.854968235566691\n",
      "*****************\n",
      "Ridge regression model, with polymoial transformation\n",
      "When lamda is 10.0, train score: 10.049055874118883 | test score: 13.476138001135917\n",
      "=============\n",
      "When lamda is 31.622776601683793, train score: 12.751706269046775 | test score: 15.82960196905122\n",
      "=============\n",
      "When lamda is 100.0, train score: 16.222690593210803 | test score: 18.980018815009977\n",
      "=============\n",
      "When lamda is 316.22776601683796, train score: 19.700253646409852 | test score: 22.068692308062715\n",
      "=============\n",
      "When lamda is 1000.0, train score: 24.287457983108887 | test score: 26.21847559440484\n",
      "=============\n",
      "When lamda is 3162.2776601683795, train score: 33.08666831985053 | test score: 34.580266018800174\n",
      "=============\n",
      "When lamda is 10000.0, train score: 46.761999359414084 | test score: 47.789520587567495\n",
      "=============\n",
      "When lamda is 31622.776601683792, train score: 62.00900191683604 | test score: 62.646288813443356\n",
      "=============\n",
      "When lamda is 100000.0, train score: 74.28758894586741 | test score: 74.73795160641836\n",
      "=============\n",
      "When lamda is 316227.7660168379, train score: 80.69255135236737 | test score: 81.07998618951191\n",
      "=============\n",
      "When lamda is 1000000.0, train score: 83.16723569778434 | test score: 83.53524235105219\n",
      "=============\n",
      "When lamda is 3162277.6601683795, train score: 84.00579580096273 | test score: 84.36776579104057\n",
      "=============\n",
      "When lamda is 10000000.0, train score: 84.2770062647736 | test score: 84.63708052069056\n",
      "=============\n"
     ]
    }
   ],
   "source": [
    "E_val_test, E_val_train = k_fold_cross_validation(10, X_2, y_2, 0.0)\n",
    "print(\"Linear regression model, with polymoial transformation\")\n",
    "print(\"train score: {}\".format(E_val_train))\n",
    "print(\"test score: {}\".format(E_val_test))\n",
    "\n",
    "print(\"*****************\")\n",
    "alpha = np.logspace(1, 7, num=13)\n",
    "print(\"Ridge regression model, with polymoial transformation\")\n",
    "for a in alpha:\n",
    "    E_val_test, E_val_train = k_fold_cross_validation(10, X_2, y_2, a)\n",
    "    print(\"When lamda is {}, train score: {} | test score: {}\".format(a, E_val_train, E_val_test))\n",
    "    print(\"=============\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The price should be: [21.89514111]\n"
     ]
    }
   ],
   "source": [
    "x_pre = np.array([5, 0.5, 2, 0, 4, 8, 4, 6, 2, 2, 2, 4, 5.5])\n",
    "x_pre = x_pre.reshape(1, 13)\n",
    "x_pre_2 = poly.fit_transform(x_pre)\n",
    "\n",
    "X_train = X_2\n",
    "y_train = y_2\n",
    "# centering the data so we do not need the intercept term (we could have also chose w_0=average y value)\n",
    "y_train_mean = np.mean(y_train)\n",
    "y_train = y_train - y_train_mean\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "w = get_coeff_ridge_normaleq(X_train, y_train, 0.0)\n",
    "y_pre = np.dot(x_pre_2, w) + y_train_mean\n",
    "print(\"The predict price should be: {}\".format(y_pre[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
