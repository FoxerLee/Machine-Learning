{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Decision Tree and Random Forest for classification\n",
    "\n",
    "In this part, we will set Decision Tree as my based algorithm, and choose Random Forest as its extension. \n",
    "\n",
    "We choose digits data in sklearn as the dataset which we used in our assignments to compare the performance between based Decision Tree and Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Build a Decision Tree with sklearn\n",
    "\n",
    "### Load digits data\n",
    "\n",
    "We used digits data from sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits['data']\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data set\n",
    "\n",
    "We splited the features into 3 periods, in order to satisfy the discrete requirement of Decesion Tree. Thanks prof. Julian Togelius suggested this idea for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, min_d, max_d, bin_size=3):\n",
    "    \n",
    "    norm_data = np.clip((data - min_d) / (max_d - min_d), 0, 1)\n",
    "    categorical_data = np.floor(bin_size*norm_data).astype(int)\n",
    "    return categorical_data\n",
    "\n",
    "\n",
    "X = preprocess(X, X.min(), X.max(), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used sklearn to split data into train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training examples:  1347\n",
      "The number of test exampels:  450\n"
     ]
    }
   ],
   "source": [
    "# We will use sklearn's method for seperating the data\n",
    "# This part of code is based on assignment 3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Looking at the train/test split\n",
    "print(\"The number of training examples: \", X_train.shape[0])\n",
    "print(\"The number of test exampels: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Decision Tree\n",
    "\n",
    "We use sklearn.tree.DecisionTreeClassifier to build our basic algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=0, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the accuracy on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data(Decision Tree): 85.111%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pre = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pre)\n",
    "\n",
    "print(\"Accuracy on the test data(Decision Tree): {0:.5}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Try Random Forest based on sklearn\n",
    "\n",
    "Then we built a Random Forest based on sklearn, to check whether there would be an improvement of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data(Random Forest): 96.222%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=50, max_features=int(1/3*len(X_train[0])))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# After training, test on the same data\n",
    "y_pre = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pre)\n",
    "\n",
    "print(\"Accuracy on the test data(Random Forest): {0:.5}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement Random Forest ourselves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Decision Tree\n",
    "\n",
    "Because Random Forest is based on Decision Tree, we need to implement Decision Tree firstly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "class decisionTree:\n",
    "    def __init__(self, depth, features_idx):\n",
    "        self.decision_tree = None\n",
    "        self.depth = depth\n",
    "        self.features_idx = features_idx\n",
    "\n",
    "    def calculateEntropy(self, y):\n",
    "        class_count = collections.defaultdict(int)\n",
    "        for yi in y:\n",
    "            class_count[yi] += 1\n",
    "        entropy = 0.0\n",
    "        for key, _ in class_count.items():\n",
    "            prob = float(class_count[key]) / len(y)\n",
    "            entropy += prob * prob\n",
    "\n",
    "        return 1 - entropy\n",
    "\n",
    "    def splitData(self, X, y, att, f):\n",
    "        sub_X = []\n",
    "        sub_y = []\n",
    "        for i, feature in enumerate(X):\n",
    "            if feature[att] == f:\n",
    "                sub_f = np.concatenate((feature[:att], feature[att + 1:]), axis=0)\n",
    "                sub_X.append(sub_f)\n",
    "                sub_y.append(y[i])\n",
    "        return np.array(sub_X), np.array(sub_y) \n",
    "\n",
    "    def buildTree(self, X, att, y, depth):\n",
    "        if np.sum(y==y[0]) == y.size:\n",
    "            return y[0]\n",
    "        if len(X[0]) == 0 or np.equal(X[0], X).all() or depth == self.depth:\n",
    "            return max(y, key=list(y).count)\n",
    "\n",
    "        features_n = len(X[0])\n",
    "        best_info = 1000\n",
    "        best_feature = -1\n",
    "\n",
    "        # find best feature\n",
    "        for i in range(features_n):\n",
    "            uni_f = set(X[:,i])\n",
    "            tmp_entropy = 0.0\n",
    "            for f in uni_f:\n",
    "                sub_X, sub_y = self.splitData(X, y, i, f)\n",
    "                tmp_entropy += len(sub_X) / float(len(X)) * self.calculateEntropy(sub_y)\n",
    "\n",
    "            if tmp_entropy < best_info:\n",
    "                best_info = tmp_entropy\n",
    "                best_feature = i\n",
    "\n",
    "        best_att = att[best_feature]\n",
    "        tree = {best_att: {}}\n",
    "\n",
    "        del (att[best_feature])\n",
    "\n",
    "        uni_f = set(X[:,best_feature])\n",
    "\n",
    "        # based on best feature to split data and go to next recursion\n",
    "        for f in uni_f:\n",
    "            sub_att = att[:]\n",
    "            sub_X, sub_y = self.splitData(X, y, best_feature, f)\n",
    "            tree[best_att][f] = self.buildTree(sub_X, sub_att, sub_y, depth+1)\n",
    "        return tree\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        categorical_data = X\n",
    "        att = [i for i in range(len(categorical_data[0]))]\n",
    "        self.decision_tree = self.buildTree(categorical_data, att, y, 0)\n",
    "\n",
    "    def traverse(self, tree, xi, att):\n",
    "        if \"{\" not in str(tree):\n",
    "            return tree\n",
    "        root = list(tree.keys())[0]\n",
    "        node = tree[root]\n",
    "        label = 0\n",
    "        for key in node.keys():\n",
    "            if xi[root] == key:\n",
    "                if type(node[key]).__name__ != 'dict':\n",
    "                    label = node[key]\n",
    "                else:\n",
    "                    label = self.traverse(node[key], xi, att)\n",
    "        return label\n",
    "\n",
    "    def predict(self, X):\n",
    "        X_test = np.array([X[:,i] for i in self.features_idx])\n",
    "        X_test = X_test.T\n",
    "        att = [i for i in range(len(X_test[0]))]\n",
    "        ans = []\n",
    "        for xi in X_test:\n",
    "            label = self.traverse(self.decision_tree, xi, att)\n",
    "            ans.append(label)\n",
    "\n",
    "        return np.array(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Random Forest\n",
    "\n",
    "Firstly, we trained our Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "forest = []\n",
    "n_estimators = 100\n",
    "max_depth = 50\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "    random_idx = random.sample([i for i in range(len(X_train))], int(2/3*len(X_train)))\n",
    "    sub_X_train = np.array([X_train[i] for i in random_idx])\n",
    "    sub_y_train = np.array([y_train[i] for i in random_idx])\n",
    "    \n",
    "    random_f_idx = random.sample([i for i in range(len(sub_X_train[0]))], int(1/3*(len(sub_X_train[0]))))\n",
    "    random_f_idx = sorted(random_f_idx)\n",
    "    sub_X_train = np.array([sub_X_train[:,i] for i in random_f_idx])\n",
    "    sub_X_train = sub_X_train.T\n",
    "     \n",
    "    tree = decisionTree(max_depth, random_f_idx)\n",
    "    tree.fit(sub_X_train, sub_y_train)\n",
    "    forest.append(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = []\n",
    "for tree in forest:\n",
    "    y_pre = tree.predict(X_test)\n",
    "    predicts.append(y_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data(Random Forest, own implement): 93.333%\n"
     ]
    }
   ],
   "source": [
    "y_pre = []\n",
    "for j in range(len(predicts[0])):\n",
    "    all_y_pre = []\n",
    "    for i in range(n_estimators):\n",
    "        all_y_pre.append(predicts[i][j])\n",
    "    y_pre.append(max(all_y_pre, key=list(all_y_pre).count))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pre)\n",
    "print(\"Accuracy on the test data(Random Forest, own implement): {0:.5}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Try different dataset -- Heart Disease UCI\n",
    "\n",
    "We also tried Heart Disease UCI to check whether there would be some improvement. The dataset is from https://www.kaggle.com/ronitf/heart-disease-uci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('heart-disease-uci.csv')\n",
    "\n",
    "data = df.values\n",
    "X = data[:, :-1]\n",
    "y = data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training examples:  227\n",
      "The number of test exampels:  76\n"
     ]
    }
   ],
   "source": [
    "# We also need to split the data\n",
    "X = preprocess(X, X.min(), X.max(), 4)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Looking at the train/test split\n",
    "print(\"The number of training examples: \", X_train.shape[0])\n",
    "print(\"The number of test exampels: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data(Decision Tree): 64.474%\n"
     ]
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(criterion='entropy', random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# After training, test on the same data\n",
    "y_pre = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pre)\n",
    "\n",
    "print(\"Accuracy on the test data(Decision Tree): {0:.5}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Random Forest\n",
    "\n",
    "We tried Random Forest based on both sklearn version and our own implements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data(Random Forest): 68.421%\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, max_depth=50, max_features=int(1/3*len(X_train[0])))\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# After training, test on the same data\n",
    "y_pre = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pre)\n",
    "\n",
    "print(\"Accuracy on the test data(Random Forest): {0:.5}%\".format(acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test data(Random Forest, own implement): 61.842%\n"
     ]
    }
   ],
   "source": [
    "forest = []\n",
    "n_estimators = 100\n",
    "max_depth = 50\n",
    "\n",
    "for _ in range(n_estimators):\n",
    "    random_idx = random.sample([i for i in range(len(X_train))], int(2/3*len(X_train)))\n",
    "    sub_X_train = np.array([X_train[i] for i in random_idx])\n",
    "    sub_y_train = np.array([y_train[i] for i in random_idx])\n",
    "    \n",
    "    random_f_idx = random.sample([i for i in range(len(sub_X_train[0]))], int(1/3*(len(sub_X_train[0]))))\n",
    "    random_f_idx = sorted(random_f_idx)\n",
    "    sub_X_train = np.array([sub_X_train[:,i] for i in random_f_idx])\n",
    "    sub_X_train = sub_X_train.T\n",
    "     \n",
    "    tree = decisionTree(max_depth, random_f_idx)\n",
    "    tree.fit(sub_X_train, sub_y_train)\n",
    "    forest.append(tree)\n",
    "    \n",
    "    \n",
    "predicts = []\n",
    "for tree in forest:\n",
    "    y_pre = tree.predict(X_test)\n",
    "    predicts.append(y_pre)\n",
    "    \n",
    "    \n",
    "y_pre = []\n",
    "for j in range(len(predicts[0])):\n",
    "    all_y_pre = []\n",
    "    for i in range(n_estimators):\n",
    "        all_y_pre.append(predicts[i][j])\n",
    "    y_pre.append(max(all_y_pre, key=list(all_y_pre).count))\n",
    "\n",
    "acc = accuracy_score(y_test, y_pre)\n",
    "print(\"Accuracy on the test data(Random Forest, own implement): {0:.5}%\".format(acc*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
