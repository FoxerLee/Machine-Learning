{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using different activation (softmax) for Neural Network\n",
    "In this part, we will change the activation of the output layer to softmax, which is more suitable for multi-class classification problem.\n",
    "\n",
    "We choose digits data in sklearn and Bank Marketing data from UCI as the datasets which we used in our assignments to compare the performance. The basic algorithm is based on the code of Assignment 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. View the implement of Neural Network in Assignment 8\n",
    "\n",
    "Firstly, we used Neural Network in Assignment 8 to see what accuracy would get for digits data.\n",
    "\n",
    "### Load digits data\n",
    "\n",
    "We used digits data from sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as r\n",
    "import random\n",
    "import keras\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data set\n",
    "\n",
    "The training features range from 0 to 15. To help the algorithm converge, we will scale the data to have a mean of 0 and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used sklearn to split data into train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training examples:  1347\n",
      "The number of test exampels:  450\n"
     ]
    }
   ],
   "source": [
    "# We will use sklearn's method for seperating the data\n",
    "# This part of code is based on assignment 3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Looking at the train/test split\n",
    "print(\"The number of training examples: \", X_train.shape[0])\n",
    "print(\"The number of test exampels: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "Our target is an integer in the range [0,..,9], so we would have 10 output neuron's in our network. We changed the label based on below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 8 9 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "def convert_y_to_vect(y):\n",
    "    y_vect = np.zeros((len(y), 10))\n",
    "    for i in range(len(y)):\n",
    "        y_vect[i, y[i]] = 1\n",
    "    return y_vect\n",
    "\n",
    "y_v_train = convert_y_to_vect(y_train)\n",
    "y_v_test = convert_y_to_vect(y_test)\n",
    "\n",
    "# A quick check to see that our code performs as we expect\n",
    "print(y_train[0:4])\n",
    "print(y_v_train[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network\n",
    "\n",
    "This part of code is almost the same as code in Assignment 8. We chose Sigmoid, and initialized weights randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The activation function and its derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_d(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "# Creating and initialing W and b\n",
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} # creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) # Return ‚Äúcontinuous uniform‚Äù random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing $\\triangledown W$ and $\\triangledown b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a forward pass throught the network. The function returns the values of  ùëé  and  ùëß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = sigmoid(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "        \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    return -(y-a_out) * sigmoid_d(z_out) \n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * sigmoid_d(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Back Propagation Algorithm. Here we used SGD instead of BGD to make training step more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25, lamb = 0.01):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        i = random.randint(0, N-1)\n",
    "        delta = {}\n",
    "        # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "        # gradient descent step\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        # loop from nl-1 to 1 backpropagating the errors\n",
    "        for l in range(len(nn_structure), 0, -1):\n",
    "            if l == len(nn_structure):\n",
    "                delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "            else:\n",
    "                if l > 1:\n",
    "                    delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            # add regularization\n",
    "            W[l] += -alpha * (1.0 * tri_W[l] + lamb/2 * W[l])\n",
    "            b[l] += -alpha * (1.0 * tri_b[l] + lamb/2 * b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0 * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Neural Network\n",
    "\n",
    "The architecture is the same as the Neural Network in Assignment 8. The input layer will have 64 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons (you can change this value).  The output layer has 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 30000, 0.25, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 83.333%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('Prediction accuracy is {0:.5}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Bank Marketing data\n",
    "\n",
    "Fisrtly, we loaded this dataset and handled categorical values. The idea was from https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('bank-additional-full.csv',sep=';')\n",
    "\n",
    "LE = LabelEncoder()\n",
    "# Work on the Categorical Values \n",
    "df['job_code'] = LE.fit_transform(df['job'])\n",
    "df['marital_code'] = LE.fit_transform(df['marital'])\n",
    "df['education_code'] = LE.fit_transform(df['education'])\n",
    "df['housing_code'] = LE.fit_transform(df['housing'])\n",
    "df['loan_code'] = LE.fit_transform(df['loan'])\n",
    "df['contact_code'] = LE.fit_transform(df['contact'])\n",
    "df['poutcome_code'] = LE.fit_transform(df['poutcome'])\n",
    "df['subscribed'] = LE.fit_transform(df['y'])\n",
    "# Drop categorical columns \n",
    "df = df.drop(['job','marital','education','housing','loan','contact','poutcome','y','day_of_week','month','default'] ,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we splited data into train data and test data. For label, we did the ont-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training examples:  30891\n",
      "The number of test exampels:  10297\n",
      "10685    1\n",
      "224      0\n",
      "29638    0\n",
      "4804     0\n",
      "Name: subscribed, dtype: int64\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_2 = df.drop('subscribed',axis=1)\n",
    "y_2 = df['subscribed']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_2)\n",
    "X_2 = scaler.transform(X_2)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, random_state=0)\n",
    "\n",
    "# Looking at the train/test split\n",
    "print(\"The number of training examples: \", X_train_2.shape[0])\n",
    "print(\"The number of test exampels: \", X_test_2.shape[0])\n",
    "\n",
    "# one-hot encoding\n",
    "num_classes = 2\n",
    "y_v_train_2 = keras.utils.to_categorical(y_train_2, num_classes)\n",
    "y_v_test_2 = keras.utils.to_categorical(y_test_2, num_classes)\n",
    "\n",
    "# A quick check to see that our code performs as we expect\n",
    "print(y_train_2[0:4])\n",
    "print(y_v_train_2[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Neural Network\n",
    "\n",
    "The input layer will have 17 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons. The output layer has 2 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [17, 30, 2]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train_2, y_v_train_2, 30000, 0.25, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (Bank Marketing data) is 88.754%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_test_2, 3)\n",
    "print('Prediction accuracy (Bank Marketing data) is {0:.5}%'.format(accuracy_score(y_test_2, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Try softmax activation based on Keras\n",
    "\n",
    "The idea of softmax is to change the output layer's activation in order to be suitable for multi-class classification problem. We used keras to implement a Neural Network and to see whether there would be some improment of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# hyperparameter. They are the same as before \n",
    "batch_size = 1\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='sigmoid', input_dim=input_shape))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1347/1347 [==============================] - 2s 2ms/step - loss: 1.5581 - accuracy: 0.6102\n",
      "Epoch 2/5\n",
      "1347/1347 [==============================] - 1s 1ms/step - loss: 0.7601 - accuracy: 0.8797\n",
      "Epoch 3/5\n",
      "1347/1347 [==============================] - 1s 1ms/step - loss: 0.4765 - accuracy: 0.9272\n",
      "Epoch 4/5\n",
      "1347/1347 [==============================] - 2s 1ms/step - loss: 0.3398 - accuracy: 0.9451\n",
      "Epoch 5/5\n",
      "1347/1347 [==============================] - 2s 2ms/step - loss: 0.2647 - accuracy: 0.9577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3514ed90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_v_train, batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 95.111%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_v_test, verbose=0)\n",
    "print('Prediction accuracy is {0:.5}%'.format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Bank Marketing data\n",
    "Because this data has only 2 classes, we changed our Neural Network's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_classes = 2\n",
    "epochs = 3\n",
    "input_shape = X_train_2.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='sigmoid', input_dim=input_shape))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Neural Network and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "30891/30891 [==============================] - 36s 1ms/step - loss: 0.2153 - accuracy: 0.9066 0s - los\n",
      "Epoch 2/3\n",
      "30891/30891 [==============================] - 43s 1ms/step - loss: 0.2028 - accuracy: 0.9093\n",
      "Epoch 3/3\n",
      "30891/30891 [==============================] - 42s 1ms/step - loss: 0.1999 - accuracy: 0.9092\n",
      "Prediction accuracy (Bank Marketing data) is 91.706%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_2, y_v_train_2, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test_2, y_v_test_2, verbose=0)\n",
    "print('Prediction accuracy (Bank Marketing data) is {0:.5}%'.format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement our own Neural Network with softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation -- sigmoid and softmax\n",
    "\n",
    "Here we implemented two activations. Sigmoid would be used in hidden layer, and softmax would be used in output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The activation function and its derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_d(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def softmax(z):\n",
    "    exps = np.exp(z - np.max(z))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def softmax_d(z):\n",
    "    return np.exp(z)/ np.sum(np.exp(z), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initialing W, b and $\\triangledown W$, $\\triangledown b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} # creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) # Return ‚Äúcontinuous uniform‚Äù random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n",
    "\n",
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward\n",
    "Here we change the output layer's activation to softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        if l == len(W): # the last layer -- output layer\n",
    "            z[l+1] = W[l].dot(node_in) + b[l]\n",
    "            a[l+1] = softmax(z[l+1])\n",
    "        else:\n",
    "            z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "            a[l+1] = sigmoid(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "        \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute $\\delta$\n",
    "\n",
    "The out layer $\\delta$ is also changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    return -(y-softmax_d(z_out))\n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * sigmoid_d(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Back Propagation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25, lamb = 0.01):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        i = random.randint(0, N-1)\n",
    "        delta = {}\n",
    "        # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "        # gradient descent step\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        # loop from nl-1 to 1 backpropagating the errors\n",
    "        for l in range(len(nn_structure), 0, -1):\n",
    "            if l == len(nn_structure):\n",
    "                delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "            else:\n",
    "                if l > 1:\n",
    "                    delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            # add regularization\n",
    "            W[l] += -alpha * (1.0 * tri_W[l] + lamb/2 * W[l])\n",
    "            b[l] += -alpha * (1.0 * tri_b[l] + lamb/2 * b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0 * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Neural Network\n",
    "\n",
    "The architecture and hyperparameters are the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 30000, 0.25, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is 88.667%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('Prediction accuracy is {0:.5}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Bank Marketing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n",
      "Prediction accuracy (Bank Marketing data) is 90.23%\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [17, 30, 2]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train_2, y_v_train_2, 30000, 0.25, 0.01)\n",
    "\n",
    "y_pred = predict_y(W, b, X_test_2, 3)\n",
    "print('Prediction accuracy (Bank Marketing data) is {0:.5}%'.format(accuracy_score(y_test_2, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
