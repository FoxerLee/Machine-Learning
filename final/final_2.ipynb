{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using different optimizers for Neural Network\n",
    "In this part, we will change the optimizers for Neural Network, which may receive a better weights updating process.\n",
    "\n",
    "We choose digits data in sklearn and Bank Marketing data from UCI as the datasets which we used in our assignments to compare the performance. The basic algorithm is based on the code of Assignment 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. View the implement of Neural Network in Assignment 8\n",
    "\n",
    "Firstly, we used Neural Network in Assignment 8 to see what accuracy would get for digits data and Fashion MNIST data.\n",
    "\n",
    "### Load digits data\n",
    "\n",
    "We used digits data from sklearn.datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.model_selection import train_test_split  \n",
    "from sklearn.metrics import accuracy_score\n",
    "import keras\n",
    "\n",
    "import numpy as np\n",
    "import numpy.random as r\n",
    "import random\n",
    "\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "y = digits.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data set\n",
    "\n",
    "The training features range from 0 to 15. To help the algorithm converge, we will scale the data to have a mean of 0 and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scale = StandardScaler()\n",
    "X = X_scale.fit_transform(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we used sklearn to split data into train data and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training examples:  1347\n",
      "The number of test exampels:  450\n"
     ]
    }
   ],
   "source": [
    "# We will use sklearn's method for seperating the data\n",
    "# This part of code is based on assignment 3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "# Looking at the train/test split\n",
    "print(\"The number of training examples: \", X_train.shape[0])\n",
    "print(\"The number of test exampels: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot encoding\n",
    "\n",
    "Our target is an integer in the range [0,..,9], so we would have 10 output neuron's in our network. We changed the label based on below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 8 9 7]\n",
      "[[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "y_v_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_v_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# A quick check to see that our code performs as we expect\n",
    "print(y_train[0:4])\n",
    "print(y_v_train[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network\n",
    "\n",
    "This part of code is almost the same as code in Assignment 8. We chose Sigmoid, and initialized weights randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The activation function and its derivative\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_d(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "\n",
    "# Creating and initialing W and b\n",
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} # creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) # Return ‚Äúcontinuous uniform‚Äù random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "    return W, b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing $\\triangledown W$ and $\\triangledown b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tri_values(nn_structure):\n",
    "    tri_W = {}\n",
    "    tri_b = {}\n",
    "    for l in range(1, len(nn_structure)):\n",
    "        tri_W[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        tri_b[l] = np.zeros((nn_structure[l],))\n",
    "    return tri_W, tri_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a forward pass throught the network. The function returns the values of  ùëé  and  ùëß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(x, W, b):\n",
    "    a = {1: x} # create a dictionary for holding the a values for all levels\n",
    "    z = { } # create a dictionary for holding the z values for all the layers\n",
    "    for l in range(1, len(W) + 1): # for each layer\n",
    "        node_in = a[l]\n",
    "        z[l+1] = W[l].dot(node_in) + b[l]  # z^(l+1) = W^(l)*a^(l) + b^(l)\n",
    "        a[l+1] = sigmoid(z[l+1]) # a^(l+1) = f(z^(l+1))\n",
    "        \n",
    "    return a, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $\\delta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_out_layer_delta(y, a_out, z_out):\n",
    "    return -(y-a_out) * sigmoid_d(z_out) \n",
    "\n",
    "def calculate_hidden_delta(delta_plus_1, w_l, z_l):\n",
    "    return np.dot(np.transpose(w_l), delta_plus_1) * sigmoid_d(z_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Back Propagation Algorithm. Here we used SGD instead of BGD to make training step more quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25, lamb = 0.01):\n",
    "    W, b = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        i = random.randint(0, N-1)\n",
    "        \n",
    "        delta = {}\n",
    "        # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "        # gradient descent step\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        # loop from nl-1 to 1 backpropagating the errors\n",
    "        for l in range(len(nn_structure), 0, -1):\n",
    "            if l == len(nn_structure):\n",
    "                delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "            else:\n",
    "                if l > 1:\n",
    "                    delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            # add regularization\n",
    "            W[l] += -alpha * (1.0 * tri_W[l] + lamb/2 * W[l])\n",
    "            b[l] += -alpha * (1.0 * tri_b[l] + lamb/2 * b[l])\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0 * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Neural Network\n",
    "\n",
    "The architecture is the same as the Neural Network in Assignment 8. The input layer will have 64 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons (you can change this value).  The output layer has 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 30000, 0.25, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (digits data) is 85.556%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('Prediction accuracy (digits data) is {0:.5}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Bank Marketing data\n",
    "\n",
    "Fisrtly, we loaded this dataset and handled categorical values. The idea was from https://becominghuman.ai/multi-layer-perceptron-mlp-models-on-real-world-banking-data-f6dd3d7e998f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('bank-additional-full.csv',sep=';')\n",
    "\n",
    "LE = LabelEncoder()\n",
    "# Work on the Categorical Values \n",
    "df['job_code'] = LE.fit_transform(df['job'])\n",
    "df['marital_code'] = LE.fit_transform(df['marital'])\n",
    "df['education_code'] = LE.fit_transform(df['education'])\n",
    "df['housing_code'] = LE.fit_transform(df['housing'])\n",
    "df['loan_code'] = LE.fit_transform(df['loan'])\n",
    "df['contact_code'] = LE.fit_transform(df['contact'])\n",
    "df['poutcome_code'] = LE.fit_transform(df['poutcome'])\n",
    "df['subscribed'] = LE.fit_transform(df['y'])\n",
    "# Drop categorical columns \n",
    "df = df.drop(['job','marital','education','housing','loan','contact','poutcome','y','day_of_week','month','default'] ,axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we splited data into train data and test data. For label, we did the ont-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of training examples:  30891\n",
      "The number of test exampels:  10297\n",
      "10685    1\n",
      "224      0\n",
      "29638    0\n",
      "4804     0\n",
      "Name: subscribed, dtype: int64\n",
      "[[0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "X_2 = df.drop('subscribed',axis=1)\n",
    "y_2 = df['subscribed']\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_2)\n",
    "X_2 = scaler.transform(X_2)\n",
    "\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, random_state=0)\n",
    "\n",
    "# Looking at the train/test split\n",
    "print(\"The number of training examples: \", X_train_2.shape[0])\n",
    "print(\"The number of test exampels: \", X_test_2.shape[0])\n",
    "\n",
    "# one-hot encoding\n",
    "num_classes = 2\n",
    "y_v_train_2 = keras.utils.to_categorical(y_train_2, num_classes)\n",
    "y_v_test_2 = keras.utils.to_categorical(y_test_2, num_classes)\n",
    "\n",
    "# A quick check to see that our code performs as we expect\n",
    "print(y_train_2[0:4])\n",
    "print(y_v_train_2[0:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Neural Network\n",
    "\n",
    "The input layer will have 17 neurons (one for each pixel in our 8 by 8 pixelated digit).  Our hidden layer has 30 neurons. The output layer has 2 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [17, 30, 2]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train_2, y_v_train_2, 30000, 0.25, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (Bank Marketing data) is 90.007%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_test_2, 3)\n",
    "print('Prediction accuracy (Bank Marketing data) is {0:.5}%'.format(accuracy_score(y_test_2, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Try RMSprop optimizer based on Keras\n",
    "\n",
    "We used keras to implement a Neural Network added RMSprop optimizer to see whether there would be some improment of accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network\n",
    "\n",
    "Firstly, we tried on digits data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# hyperparameter. They are the same as before \n",
    "batch_size = 1\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='sigmoid', input_dim=input_shape))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.RMSprop(rho=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1347/1347 [==============================] - 2s 1ms/step - loss: 1.4365 - accuracy: 0.6971\n",
      "Epoch 2/5\n",
      "1347/1347 [==============================] - 2s 1ms/step - loss: 0.4932 - accuracy: 0.9065\n",
      "Epoch 3/5\n",
      "1347/1347 [==============================] - 2s 1ms/step - loss: 0.2652 - accuracy: 0.9332\n",
      "Epoch 4/5\n",
      "1347/1347 [==============================] - 2s 1ms/step - loss: 0.1859 - accuracy: 0.9488\n",
      "Epoch 5/5\n",
      "1347/1347 [==============================] - 3s 2ms/step - loss: 0.1422 - accuracy: 0.9614\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3183e590>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_v_train, batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (digits data) is 95.111%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_v_test, verbose=0)\n",
    "print('Prediction accuracy (digits data) is {0:.5}%'.format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Bank Marketing data\n",
    "Because this data has only 2 classes, we changed our Neural Network's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_classes = 2\n",
    "epochs = 3\n",
    "input_shape = X_train_2.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='sigmoid', input_dim=input_shape))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.RMSprop(rho=0.9),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Neural Network and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "30891/30891 [==============================] - 62s 2ms/step - loss: 0.2415 - accuracy: 0.9048\n",
      "Epoch 2/3\n",
      "30891/30891 [==============================] - 58s 2ms/step - loss: 0.2406 - accuracy: 0.9069\n",
      "Epoch 3/3\n",
      "30891/30891 [==============================] - 66s 2ms/step - loss: 0.2434 - accuracy: 0.9069\n",
      "Prediction accuracy (Bank Marketing data) is 91.201%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_2, y_v_train_2, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test_2, y_v_test_2, verbose=0)\n",
    "print('Prediction accuracy (Bank Marketing data) is {0:.5}%'.format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Implement our own RMSprop optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and initialing W, b\n",
    "\n",
    "Here we added two more matrixes called W_v and b_v, to store momentum, which would affect the gradient direction of this step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_and_init_weights(nn_structure):\n",
    "    W = {} # creating a dictionary i.e. a set of key: value pairs\n",
    "    b = {}\n",
    "    W_v = {}\n",
    "    b_v = {}\n",
    "    \n",
    "    for l in range(1, len(nn_structure)):\n",
    "        W[l] = r.random_sample((nn_structure[l], nn_structure[l-1])) # Return ‚Äúcontinuous uniform‚Äù random floats in the half-open interval [0.0, 1.0). \n",
    "        b[l] = r.random_sample((nn_structure[l],))\n",
    "        # initalize as zero\n",
    "        W_v[l] = np.zeros((nn_structure[l], nn_structure[l-1]))\n",
    "        b_v[l] = np.zeros((nn_structure[l],))\n",
    "    return W, b, W_v, b_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Back Propagation Algorithm\n",
    "\n",
    "Here we used SGD instead of BGD to make training step more quickly. And we added RMSprop optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.001, lamb = 0.01, momentum=0.9):\n",
    "    W, b, W_v, b_v = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "            \n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        # SGD\n",
    "        i = random.randint(0, N-1)\n",
    "        delta = {}\n",
    "        # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "        # gradient descent step\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        # loop from nl-1 to 1 backpropagating the errors\n",
    "        for l in range(len(nn_structure), 0, -1):\n",
    "            if l == len(nn_structure):\n",
    "                delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "            else:\n",
    "                if l > 1:\n",
    "                    delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W_v[l] *= 0.99\n",
    "            W_v[l] += (1 - 0.99) * tri_W[l] * tri_W[l]\n",
    "        \n",
    "            b_v[l] *= 0.99\n",
    "            b_v[l] += (1 - 0.99) * tri_b[l] * tri_b[l]\n",
    "\n",
    "            W[l] -= alpha * 1.0 * tri_W[l] / (np.sqrt(W_v[l]) + 0.5)\n",
    "            b[l] -= alpha * 1.0 * tri_b[l] / (np.sqrt(b_v[l]) + 0.5)\n",
    "\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0 * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Neural Network\n",
    "\n",
    "The architecture and hyperparameters are the same as before. Firstly we tried on digits data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 30000, 0.25, 0.01, 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (digits data) is 96.0%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('Prediction accuracy (digits data) is {0:.5}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Bank Marketing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n",
      "Prediction accuracy (Bank Marketing data) is 90.968%\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [17, 30, 2]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train_2, y_v_train_2, 30000, 0.25, 0.01)\n",
    "\n",
    "y_pred = predict_y(W, b, X_test_2, 3)\n",
    "print('Prediction accuracy (Bank Marketing data) is {0:.5}%'.format(accuracy_score(y_test_2, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Try AdaGrad optimizer based on Keras\n",
    "\n",
    "We tried another optimizer called AdaGrad. Firstly, we used Keras to check whether it would receive some improvement of accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# hyperparameter. They are the same as before \n",
    "batch_size = 1\n",
    "num_classes = 10\n",
    "epochs = 5\n",
    "input_shape = X_train.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='sigmoid', input_dim=input_shape))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adagrad(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1347/1347 [==============================] - 5s 4ms/step - loss: 1.7160 - accuracy: 0.6956\n",
      "Epoch 2/5\n",
      "1347/1347 [==============================] - 5s 4ms/step - loss: 1.2496 - accuracy: 0.8790\n",
      "Epoch 3/5\n",
      "1347/1347 [==============================] - 5s 4ms/step - loss: 1.0100 - accuracy: 0.9020\n",
      "Epoch 4/5\n",
      "1347/1347 [==============================] - 5s 4ms/step - loss: 0.8509 - accuracy: 0.9191\n",
      "Epoch 5/5\n",
      "1347/1347 [==============================] - 5s 4ms/step - loss: 0.7387 - accuracy: 0.9332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a336046d0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_v_train, batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (digits data) is 93.111%\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_v_test, verbose=0)\n",
    "print('Prediction accuracy (digits data) is {0:.5}%'.format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Bank Marketing data\n",
    "\n",
    "Because this data has only 2 classes, we changed our Neural Network's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "num_classes = 2\n",
    "epochs = 5\n",
    "input_shape = X_train_2.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, activation='sigmoid', input_dim=input_shape))\n",
    "model.add(Dense(num_classes, activation='sigmoid'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adagrad(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Neural Network and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30891/30891 [==============================] - 55s 2ms/step - loss: 0.2207 - accuracy: 0.9043\n",
      "Epoch 2/5\n",
      "30891/30891 [==============================] - 60s 2ms/step - loss: 0.2058 - accuracy: 0.9087\n",
      "Epoch 3/5\n",
      "30891/30891 [==============================] - 55s 2ms/step - loss: 0.2039 - accuracy: 0.9095 0s - loss: 0.2040 - accura\n",
      "Epoch 4/5\n",
      "30891/30891 [==============================] - 54s 2ms/step - loss: 0.2031 - accuracy: 0.9098 0s - loss: 0.2030 - accu\n",
      "Epoch 5/5\n",
      "30891/30891 [==============================] - 56s 2ms/step - loss: 0.2025 - accuracy: 0.9101\n",
      "Prediction accuracy (Bank Marketing data) is 91.658%\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_2, y_v_train_2, batch_size=batch_size, epochs=epochs, verbose=1)\n",
    "\n",
    "score = model.evaluate(X_test_2, y_v_test_2, verbose=0)\n",
    "print('Prediction accuracy (Bank Marketing data) is {0:.5}%'.format(score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implement our own Optimizer with AdaGrad\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the Back Propagation Algorithm\n",
    "\n",
    "Here we changed the code in gradient descent step to AdaGrad optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(nn_structure, X, y, iter_num=3000, alpha=0.25, lamb = 0.01):\n",
    "    W, b, W_v, b_v = setup_and_init_weights(nn_structure)\n",
    "    cnt = 0\n",
    "    N = len(y)\n",
    "    avg_cost_func = []\n",
    "    print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "    while cnt < iter_num:\n",
    "        if cnt%1000 == 0:\n",
    "            print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "        tri_W, tri_b = init_tri_values(nn_structure)\n",
    "        avg_cost = 0\n",
    "        i = random.randint(0, N-1)\n",
    "        delta = {}\n",
    "        # perform the feed forward pass and return the stored a and z values, to be used in the\n",
    "        # gradient descent step\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        # loop from nl-1 to 1 backpropagating the errors\n",
    "        for l in range(len(nn_structure), 0, -1):\n",
    "            if l == len(nn_structure):\n",
    "                delta[l] = calculate_out_layer_delta(y[i,:], a[l], z[l])\n",
    "                avg_cost += np.linalg.norm((y[i,:]-a[l]))\n",
    "            else:\n",
    "                if l > 1:\n",
    "                    delta[l] = calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                tri_W[l] += np.dot(delta[l+1][:,np.newaxis], np.transpose(a[l][:,np.newaxis]))# np.newaxis increase the number of dimensions\n",
    "                tri_b[l] += delta[l+1]\n",
    "        # perform the gradient descent step for the weights in each layer\n",
    "        for l in range(len(nn_structure) - 1, 0, -1):\n",
    "            W_v[l] += tri_W[l]*tri_W[l]         \n",
    "            b_v[l] += tri_b[l]*tri_b[l]\n",
    "            \n",
    "            W[l] -= alpha * tri_W[l] / (np.sqrt(W_v[l]) + 1e-7)\n",
    "            b[l] -= alpha * tri_b[l] / (np.sqrt(b_v[l]) + 1e-7)\n",
    "        # complete the average cost calculation\n",
    "        avg_cost = 1.0 * avg_cost\n",
    "        avg_cost_func.append(avg_cost)\n",
    "        cnt += 1\n",
    "    return W, b, avg_cost_func\n",
    "\n",
    "\n",
    "def predict_y(W, b, X, n_layers):\n",
    "    N = X.shape[0]\n",
    "    y = np.zeros((N,))\n",
    "    for i in range(N):\n",
    "        a, z = feed_forward(X[i, :], W, b)\n",
    "        y[i] = np.argmax(a[n_layers])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the Neural Network\n",
    "\n",
    "The architecture and hyperparameters are the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [64, 30, 10]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train, y_v_train, 30000, 0.25, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy (digits data) is 96.889%\n"
     ]
    }
   ],
   "source": [
    "y_pred = predict_y(W, b, X_test, 3)\n",
    "print('Prediction accuracy (digits data) is {0:.5}%'.format(accuracy_score(y_test, y_pred) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try on Bank Marketing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 30000 iterations\n",
      "Iteration 0 of 30000\n",
      "Iteration 1000 of 30000\n",
      "Iteration 2000 of 30000\n",
      "Iteration 3000 of 30000\n",
      "Iteration 4000 of 30000\n",
      "Iteration 5000 of 30000\n",
      "Iteration 6000 of 30000\n",
      "Iteration 7000 of 30000\n",
      "Iteration 8000 of 30000\n",
      "Iteration 9000 of 30000\n",
      "Iteration 10000 of 30000\n",
      "Iteration 11000 of 30000\n",
      "Iteration 12000 of 30000\n",
      "Iteration 13000 of 30000\n",
      "Iteration 14000 of 30000\n",
      "Iteration 15000 of 30000\n",
      "Iteration 16000 of 30000\n",
      "Iteration 17000 of 30000\n",
      "Iteration 18000 of 30000\n",
      "Iteration 19000 of 30000\n",
      "Iteration 20000 of 30000\n",
      "Iteration 21000 of 30000\n",
      "Iteration 22000 of 30000\n",
      "Iteration 23000 of 30000\n",
      "Iteration 24000 of 30000\n",
      "Iteration 25000 of 30000\n",
      "Iteration 26000 of 30000\n",
      "Iteration 27000 of 30000\n",
      "Iteration 28000 of 30000\n",
      "Iteration 29000 of 30000\n",
      "Prediction accuracy (Bank Marketing data) is 91.541%\n"
     ]
    }
   ],
   "source": [
    "nn_structure = [17, 30, 2]\n",
    "    \n",
    "# train the NN\n",
    "W, b, avg_cost_func = train_nn(nn_structure, X_train_2, y_v_train_2, 30000, 0.25, 0.01)\n",
    "\n",
    "y_pred = predict_y(W, b, X_test_2, 3)\n",
    "print('Prediction accuracy (Bank Marketing data) is {0:.5}%'.format(accuracy_score(y_test_2, y_pred) * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
